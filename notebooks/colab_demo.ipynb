{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedMCQA Robustness Study - Google Colab Demo\n",
    "\n",
    "This notebook demonstrates running the Medical MCQ Robustness experiments on Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 for 4B model, A100 for 27B model)\n",
    "- Hugging Face account with access to MedGemma models\n",
    "\n",
    "**Note:** MedGemma requires accepting the license at https://huggingface.co/google/medgemma-4b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets bitsandbytes accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face (required for MedGemma access)\n",
    "from huggingface_hub import login\n",
    "login()  # This will prompt for your HF token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MedGemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Choose model based on available GPU memory\n",
    "# - T4 (16GB): Use 4B model\n",
    "# - A100 (40GB): Can use 27B with 4-bit quantization (but has issues, see below)\n",
    "# - A100 (80GB): Can use 27B full precision\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"  # Use 4B for T4 GPU\n",
    "# MODEL_ID = \"google/medgemma-27b-text-it\"  # Use 27B for A100 80GB\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# For 4B model on T4\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# NOTE: For 27B model, do NOT use 4-bit quantization - it produces NaN logits!\n",
    "# Use full precision bfloat16 on A100 80GB instead:\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model with a Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_new_tokens=50):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    new_tokens = outputs[0, inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Test query\n",
    "response = generate_response(\"What is the mechanism of action of aspirin?\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load MedMCQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MedMCQA validation set\n",
    "dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\")\n",
    "print(f\"Loaded {len(dataset)} questions\")\n",
    "\n",
    "# Show a sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nSample question:\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"A: {sample['opa']}\")\n",
    "print(f\"B: {sample['opb']}\")\n",
    "print(f\"C: {sample['opc']}\")\n",
    "print(f\"D: {sample['opd']}\")\n",
    "print(f\"Correct: {['A', 'B', 'C', 'D'][sample['cop']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run MCQ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def format_mcq_prompt(item, style=\"zero_shot\"):\n",
    "    \"\"\"Format a MedMCQA item into a prompt.\"\"\"\n",
    "    question = item['question']\n",
    "    options = [\n",
    "        f\"A. {item['opa']}\",\n",
    "        f\"B. {item['opb']}\",\n",
    "        f\"C. {item['opc']}\",\n",
    "        f\"D. {item['opd']}\"\n",
    "    ]\n",
    "    \n",
    "    if style == \"zero_shot\":\n",
    "        prompt = f\"\"\"Answer the following medical question by selecting the correct option.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{chr(10).join(options)}\n",
    "\n",
    "Answer with just the letter (A, B, C, or D):\"\"\"\n",
    "    elif style == \"zero_shot_cot\":\n",
    "        prompt = f\"\"\"Answer the following medical question. Think step by step.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{chr(10).join(options)}\n",
    "\n",
    "Let's think step by step, then provide the answer as a single letter (A, B, C, or D):\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def parse_answer(response):\n",
    "    \"\"\"Extract the answer letter from model response.\"\"\"\n",
    "    response = response.strip().upper()\n",
    "    \n",
    "    # Direct letter match\n",
    "    if response and response[0] in 'ABCD':\n",
    "        return response[0]\n",
    "    \n",
    "    # Look for patterns like \"The answer is A\" or \"A.\"\n",
    "    patterns = [\n",
    "        r'answer\\s*(?:is)?\\s*([A-D])',\n",
    "        r'([A-D])\\s*[.)]',\n",
    "        r'\\b([A-D])\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    return \"unknown\"\n",
    "\n",
    "def evaluate_mcq(dataset, num_samples=20, style=\"zero_shot\"):\n",
    "    \"\"\"Evaluate model on MCQ dataset.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset.select(range(num_samples)))):\n",
    "        prompt = format_mcq_prompt(item, style)\n",
    "        response = generate_response(prompt, max_new_tokens=100 if style == \"zero_shot_cot\" else 20)\n",
    "        predicted = parse_answer(response)\n",
    "        expected = ['A', 'B', 'C', 'D'][item['cop']]\n",
    "        \n",
    "        is_correct = predicted == expected\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        results.append({\n",
    "            'question': item['question'][:50] + '...',\n",
    "            'predicted': predicted,\n",
    "            'expected': expected,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, results\n",
    "\n",
    "# Run evaluation on a small sample\n",
    "print(\"Evaluating on 20 samples...\")\n",
    "accuracy, results = evaluate_mcq(dataset, num_samples=20, style=\"zero_shot\")\n",
    "print(f\"\\nAccuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment: Option Order Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_options(item, seed=42):\n",
    "    \"\"\"Shuffle options and return new item with mapping.\"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    options = [\n",
    "        ('A', item['opa']),\n",
    "        ('B', item['opb']),\n",
    "        ('C', item['opc']),\n",
    "        ('D', item['opd'])\n",
    "    ]\n",
    "    \n",
    "    original_answer = ['A', 'B', 'C', 'D'][item['cop']]\n",
    "    \n",
    "    # Shuffle\n",
    "    shuffled = options.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    \n",
    "    # Create mapping and find new answer position\n",
    "    new_answer = None\n",
    "    for new_pos, (orig_letter, text) in enumerate(shuffled):\n",
    "        if orig_letter == original_answer:\n",
    "            new_answer = ['A', 'B', 'C', 'D'][new_pos]\n",
    "            break\n",
    "    \n",
    "    # Create new item\n",
    "    new_item = item.copy()\n",
    "    new_item['opa'] = shuffled[0][1]\n",
    "    new_item['opb'] = shuffled[1][1]\n",
    "    new_item['opc'] = shuffled[2][1]\n",
    "    new_item['opd'] = shuffled[3][1]\n",
    "    new_item['cop'] = ['A', 'B', 'C', 'D'].index(new_answer)\n",
    "    \n",
    "    return new_item\n",
    "\n",
    "# Test option order sensitivity\n",
    "print(\"Testing option order sensitivity on 10 samples...\")\n",
    "\n",
    "consistency_results = []\n",
    "for i, item in enumerate(tqdm(dataset.select(range(10)))):\n",
    "    # Original order\n",
    "    prompt_orig = format_mcq_prompt(item)\n",
    "    response_orig = generate_response(prompt_orig, max_new_tokens=20)\n",
    "    pred_orig = parse_answer(response_orig)\n",
    "    \n",
    "    # Shuffled order\n",
    "    shuffled_item = shuffle_options(item, seed=i)\n",
    "    prompt_shuffled = format_mcq_prompt(shuffled_item)\n",
    "    response_shuffled = generate_response(prompt_shuffled, max_new_tokens=20)\n",
    "    pred_shuffled = parse_answer(response_shuffled)\n",
    "    \n",
    "    # Check if predictions are consistent (same content, different letter is OK)\n",
    "    expected_orig = ['A', 'B', 'C', 'D'][item['cop']]\n",
    "    expected_shuffled = ['A', 'B', 'C', 'D'][shuffled_item['cop']]\n",
    "    \n",
    "    orig_correct = pred_orig == expected_orig\n",
    "    shuffled_correct = pred_shuffled == expected_shuffled\n",
    "    \n",
    "    consistency_results.append({\n",
    "        'original_pred': pred_orig,\n",
    "        'shuffled_pred': pred_shuffled,\n",
    "        'original_correct': orig_correct,\n",
    "        'shuffled_correct': shuffled_correct,\n",
    "        'consistent': orig_correct == shuffled_correct\n",
    "    })\n",
    "\n",
    "df_consistency = pd.DataFrame(consistency_results)\n",
    "print(f\"\\nOriginal accuracy: {df_consistency['original_correct'].mean():.1%}\")\n",
    "print(f\"Shuffled accuracy: {df_consistency['shuffled_correct'].mean():.1%}\")\n",
    "print(f\"Consistency rate: {df_consistency['consistent'].mean():.1%}\")\n",
    "display(df_consistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load PubMedQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PubMedQA\n",
    "pubmedqa = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", split=\"train\")\n",
    "print(f\"Loaded {len(pubmedqa)} PubMedQA questions\")\n",
    "\n",
    "# Show sample\n",
    "sample = pubmedqa[0]\n",
    "print(f\"\\nQuestion: {sample['question']}\")\n",
    "print(f\"Answer: {sample['final_decision']}\")\n",
    "print(f\"Context: {sample['context']['contexts'][0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pubmedqa(dataset, num_samples=10, use_context=True):\n",
    "    \"\"\"Evaluate on PubMedQA.\"\"\"\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    for item in tqdm(dataset.select(range(num_samples))):\n",
    "        if use_context:\n",
    "            context = \" \".join(item['context']['contexts'])\n",
    "            prompt = f\"\"\"Based on the following context, answer the research question.\n",
    "\n",
    "Context: {context[:1500]}\n",
    "\n",
    "Question: {item['question']}\n",
    "\n",
    "Answer with one word: yes, no, or maybe.\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Based on your medical knowledge, answer the following research question.\n",
    "\n",
    "Question: {item['question']}\n",
    "\n",
    "Answer with one word: yes, no, or maybe.\"\"\"\n",
    "        \n",
    "        response = generate_response(prompt, max_new_tokens=20)\n",
    "        response_lower = response.strip().lower()\n",
    "        \n",
    "        # Parse answer\n",
    "        if 'yes' in response_lower:\n",
    "            predicted = 'yes'\n",
    "        elif 'no' in response_lower:\n",
    "            predicted = 'no'\n",
    "        elif 'maybe' in response_lower:\n",
    "            predicted = 'maybe'\n",
    "        else:\n",
    "            predicted = 'unknown'\n",
    "        \n",
    "        expected = item['final_decision']\n",
    "        is_correct = predicted == expected\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        \n",
    "        results.append({\n",
    "            'question': item['question'][:50] + '...',\n",
    "            'predicted': predicted,\n",
    "            'expected': expected,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "    \n",
    "    accuracy = correct / num_samples\n",
    "    return accuracy, results\n",
    "\n",
    "# Test with and without context\n",
    "print(\"Evaluating PubMedQA WITH context...\")\n",
    "acc_with_ctx, res_with = evaluate_pubmedqa(pubmedqa, num_samples=10, use_context=True)\n",
    "print(f\"Accuracy with context: {acc_with_ctx:.1%}\")\n",
    "\n",
    "print(\"\\nEvaluating PubMedQA WITHOUT context...\")\n",
    "acc_without_ctx, res_without = evaluate_pubmedqa(pubmedqa, num_samples=10, use_context=False)\n",
    "print(f\"Accuracy without context: {acc_without_ctx:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading MedGemma models on Colab\n",
    "2. Evaluating on MedMCQA multiple choice questions\n",
    "3. Testing option order sensitivity\n",
    "4. Evaluating on PubMedQA with/without context\n",
    "\n",
    "**Key Findings:**\n",
    "- MedGemma-4B works well on T4 GPU\n",
    "- MedGemma-27B requires A100 80GB with full precision (4-bit quantization has NaN issues)\n",
    "- Option shuffling can affect model predictions\n",
    "- Context generally improves PubMedQA performance"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
