@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group}
}

@article{nori2023capabilities,
  title={Capabilities of GPT-4 on medical challenge problems},
  author={Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
  journal={arXiv preprint arXiv:2303.13375},
  year={2023}
}

@article{medgemma2024,
  title={MedGemma: Medical Language Models},
  author={Google DeepMind},
  journal={Google AI Blog},
  year={2024},
  note={Technical report}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{pal2022medmcqa,
  title={MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  journal={Proceedings of the Conference on Health, Inference, and Learning},
  pages={248--260},
  year={2022}
}

@article{jin2019pubmedqa,
  title={PubMedQA: A dataset for biomedical research question answering},
  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
  journal={Proceedings of EMNLP-IJCNLP},
  pages={2567--2577},
  year={2019}
}

@article{lu2022fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={Proceedings of ACL},
  pages={8086--8098},
  year={2022}
}

@article{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={Proceedings of ICML},
  pages={12697--12706},
  year={2021}
}

@article{wang2023large,
  title={Large language models are not yet human-level reasoners},
  author={Wang, Boshi and Deng, Xiang and Sun, Huan},
  journal={Findings of EMNLP},
  year={2023}
}

@article{chen2023evaluating,
  title={Evaluating large language models on medical evidence summarization},
  author={Chen, Liyan and others},
  journal={npj Digital Medicine},
  volume={6},
  number={1},
  pages={158},
  year={2023}
}

@article{jin2024hidden,
  title={Hidden flaws behind expert-level accuracy of multimodal GPT-4 vision in medicine},
  author={Jin, Qiao and Chen, Fangyuan and Zhou, Yiliang and Xu, Ziyang and Cheung, Justin and Chen, Robert and Summers, Ronald and others},
  journal={npj Digital Medicine},
  volume={7},
  number={1},
  pages={190},
  year={2024}
}

@inproceedings{zheng2024llm,
  title={Large Language Models Are Not Robust Multiple Choice Selectors},
  author={Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2024},
  note={Spotlight paper}
}

@inproceedings{prosa2024,
  title={ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs},
  author={Zhuo, Jingming and Xing, Shuang and Hu, Zixuan and Wang, Zhonghai and Zhai, Guangtao and Zhang, Xiao-Ping},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={25372--25410},
  year={2024}
}

@article{sprague2024mind,
  title={Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse},
  author={Sprague, Zayne and Pei, Jiasheng and Chaturvedi, Akari and Lee, Zeyao and Gao, Nan and Chen, Yige and Zhang, Rui},
  journal={arXiv preprint arXiv:2410.21333},
  year={2024}
}

@techreport{meincke2024cot,
  title={The Decreasing Value of Chain of Thought in Prompting},
  author={Meincke, Lennart and Mollick, Ethan R and Mollick, Lilach and Shapiro, Dan},
  institution={Wharton Generative AI Labs},
  year={2024},
  note={SSRN 5285532}
}

@article{omar2024cotmedical,
  title={A comparative evaluation of chain-of-thought-based prompt engineering techniques for medical question answering},
  author={Omar, Rana and others},
  journal={Computers in Biology and Medicine},
  year={2024},
  publisher={Elsevier}
}

@article{barnett2024rag,
  title={Seven Failure Points When Engineering a Retrieval Augmented Generation System},
  author={Barnett, Scott and Kurniawan, Stefanus and Thudumu, Srikanth and Brannelly, Zach and Abdelrazek, Mohamed},
  journal={arXiv preprint arXiv:2401.05856},
  year={2024},
  doi={10.48550/arXiv.2401.05856}
}

@article{liu2024lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024}
}

@article{singhal2023medpalm2,
  title={Towards Expert-Level Medical Question Answering with Large Language Models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and others},
  journal={arXiv preprint arXiv:2305.09617},
  year={2023}
}

@article{wang2023selfconsistency,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={Proceedings of ICLR},
  year={2023}
}

@article{labrak2024biomistral,
  title={BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains},
  author={Labrak, Yanis and Bazoge, Adrien and Morin, Emmanuel and Gourraud, Pierre-Antoine and Rouvier, Mickael and Dufour, Richard},
  journal={arXiv preprint arXiv:2402.10373},
  year={2024}
}

@inproceedings{xiong2024benchmarking,
  title={Benchmarking Retrieval-Augmented Generation for Medicine},
  author={Xiong, Guangzhi and Jin, Qiao and Lu, Zhiyong and Zhang, Aidong},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  pages={6233--6251},
  year={2024},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  doi={10.18653/v1/2024.findings-acl.372},
  url={https://aclanthology.org/2024.findings-acl.372/}
}

@article{wu2024clasheval,
  title={ClashEval: Quantifying the tug-of-war between an {LLM}'s prior knowledge and external evidence},
  author={Wu, Kevin and Wu, Eric and Zou, James},
  journal={arXiv preprint arXiv:2404.10198},
  year={2024},
  doi={10.48550/arXiv.2404.10198}
}

@article{sohn2024rag2,
  title={{RAG}$^2$: A Full-Stack Framework for Reliable Medical {RAG}},
  author={Sohn, Kyungwoo and Hong, Sungjin and Guevara, Carolina and Amrollahi, Reza and Gholami, Ali and Niu, Han and Mitra, Bhaskar},
  journal={arXiv preprint arXiv:2411.00300},
  year={2024},
  doi={10.48550/arXiv.2411.00300}
}
