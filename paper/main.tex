\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

\title{When Chain-of-Thought Backfires: Evaluating Prompt Sensitivity in Medical Language Models}

\author{
  Binesh Sadanandan\\
  University of New Haven\\
  \texttt{bsada1@unh.newhaven.edu}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Large language models are increasingly deployed in medical settings, yet their sensitivity to prompt formatting remains poorly characterized. We evaluate MedGemma (4B and 27B variants) on MedMCQA (4,183 questions) and PubMedQA (1,000 questions). Our experiments reveal concerning findings: chain-of-thought prompting \textit{decreases} accuracy by 5.7\% compared to direct answering; few-shot examples degrade performance by 11.9\% while increasing position bias from 0.14 to 0.47; shuffling answer options causes the model to change predictions 59.1\% of the time with accuracy dropping up to 27.4 percentage points; and truncating context to 50\% causes accuracy to plummet below the no-context baseline. These results demonstrate that prompt engineering techniques validated on general-purpose models do not transfer to domain-specific medical LLMs.
\end{abstract}

\section{Introduction}

Large language models have achieved impressive performance on medical licensing exams, with GPT-4 exceeding passing thresholds by over 20 points \citep{nori2023capabilities} and Med-PaLM 2 reaching 86.5\% on MedQA \citep{singhal2023medpalm2}. These results have fueled enthusiasm for deploying LLMs in clinical decision support. However, benchmark accuracy tells only part of the story---how models respond to variations in prompt format, question ordering, and context presentation remains poorly understood, despite being critical for real-world deployment where inputs are rarely formatted identically to benchmark conditions.

We focus on MedGemma \citep{medgemma2025}, Google's medical-specialist LLM built on the Gemma architecture. A widely held belief in the LLM community is that certain prompting strategies reliably improve performance. Chain-of-thought prompting, which instructs models to reason step-by-step before answering, has shown consistent gains on mathematical and logical reasoning tasks \citep{wei2022chain}. Few-shot learning, where examples are provided in-context, helps models understand desired output formats \citep{brown2020language}. These techniques are often treated as ``best practices'' that should transfer across domains and model families.

But should they? Domain-specific models may have internalized different patterns during training. A model trained extensively on medical literature might already encode structured clinical reasoning, making explicit chain-of-thought prompts redundant or even counterproductive. Similarly, few-shot examples drawn from one medical specialty might prime the model with concepts that are irrelevant or misleading for questions in other specialties.

We present a systematic evaluation of MedGemma's sensitivity to prompt variations across three experimental conditions. First, we conduct a prompt ablation study comparing zero-shot, chain-of-thought, and few-shot strategies on 4,183 MedMCQA questions, measuring both accuracy and position bias. Second, we test option order sensitivity by shuffling answer choices and measuring how often the model changes its prediction---a direct test of whether responses reflect semantic understanding or superficial position cues. Third, we evaluate evidence conditioning on 1,000 PubMedQA questions, systematically varying context completeness to understand how partial information affects accuracy. Our findings challenge conventional assumptions about prompt engineering in medical AI and have important implications for clinical deployment.

\section{Related Work}

\paragraph{Medical Language Models and Benchmarks.} Medical language models often demonstrate impressive benchmark scores on exam-style question answering. For example, GPT-4 performs strongly on medical challenge sets \citep{nori2023capabilities}, and Med-PaLM and Med-PaLM 2 report high scores on MedQA \citep{singhal2023large, singhal2023medpalm2}. Open, domain-tuned models have also emerged, including MedGemma \citep{medgemma2025} and BioMistral \citep{labrak2024biomistral}. However, most reporting still emphasizes a single headline accuracy, while deployment inputs vary in formatting, context quality, and answer presentation.

\paragraph{Prompting for Reasoning.} Chain-of-thought (CoT) prompting can improve performance on general reasoning tasks by eliciting intermediate steps \citep{wei2022chain}. Follow-up work such as self-consistency explores sampling multiple reasoning paths and aggregating answers \citep{wang2023selfconsistency}. These techniques are now common defaults, despite their added token cost and their sensitivity to output parsing.

\paragraph{When CoT Backfires.} Recent work challenges the idea that CoT helps everywhere. \citet{sprague2024mind} identify settings where step-by-step prompting reduces accuracy, connecting these failures to cases where deliberate reasoning hurts humans as well. Meincke et al.\ argue that the gains from CoT have diminished for newer models and can even reverse on some tasks \citep{meincke2024cot}. In medical question answering, Omar et al.\ compare multiple CoT-style prompts and find that improvements depend on the specific method and dataset, rather than following a simple monotonic trend \citep{omar2024cotmedical}.

\paragraph{Prompt Sensitivity and Few-shot Formatting.} Even without CoT, small prompt changes can shift performance. \citet{lu2022fantastically} show that few-shot example order can materially affect accuracy, and \citet{zhao2021calibrate} propose calibration methods that reduce sensitivity to label and prompt priors. ProSA provides a more systematic view by measuring how model outputs vary across prompt templates \citep{prosa2024}. Together, this work suggests that comparisons between models can be misleading if prompt choices are not controlled.

\paragraph{Multiple-choice Artifacts.} Multiple-choice evaluation introduces its own failure modes. \citet{zheng2024llm} show that LLMs exhibit selection bias, preferring certain option identifiers even when content is balanced. Our option reordering experiments build on this line of work by separating changes in answer position from changes in distractor content.

\paragraph{Retrieval and Context Quality.} Retrieval-augmented generation (RAG) augments a model with external documents at inference time. In medicine, benchmarking work finds large variation in RAG performance across retrievers and corpora, and it reports a pronounced ``lost-in-the-middle'' effect in biomedical settings \citep{xiong2024benchmarking, liu2024lost}. Retrieval can also introduce new failure modes. ClashEval shows that models can be led astray by incorrect retrieved context, even when it conflicts with the question \citep{wu2024clasheval}. Recent medical RAG methods aim to improve reliability under imperfect retrieval, for example by training models to cite supporting evidence and by evaluating failure cases explicitly \citep{sohn2024rag2, barnett2024rag}.

\paragraph{Summary.} Our work complements prior studies by focusing on a single medical model family and quantifying how concrete prompt and context variations affect both accuracy and bias, rather than reporting only peak performance under one prompt choice.

\section{Methods}

\subsection{Models and Datasets}

We evaluate MedGemma-4B, the 4-billion parameter instruction-tuned variant at bfloat16 precision, and MedGemma-27B, the 27-billion parameter model requiring full bfloat16 precision on 80GB A100 GPUs. Initial experiments with 4-bit quantization on the 27B model produced NaN logits---a notable finding suggesting that quantization techniques validated on general models may not transfer to medical-specialist architectures.

We use two standard medical QA benchmarks. MedMCQA \citep{pal2022medmcqa} contains questions from Indian medical entrance examinations across 21 subjects; we use the 4,183-question validation split. PubMedQA \citep{jin2019pubmedqa} contains research questions derived from PubMed titles that must be answered using abstracts; we use the 1,000-question labeled subset.

\subsection{Experimental Conditions}

\paragraph{Experiment 1: Prompt Ablation.} We test five prompting strategies on MedMCQA: (1) zero-shot direct, presenting the question and requesting only the answer letter; (2) zero-shot CoT, adding ``think step by step''; (3) few-shot direct, providing three example Q\&A pairs; (4) few-shot CoT, providing three examples with reasoning traces; and (5) answer-only, a minimal prompt with no instructions.

\paragraph{Experiment 2: Option Order Sensitivity.} We apply five transformations to each question: original order, random shuffle, rotate-1 (cyclic shift by one), rotate-2 (shift by two), and distractor swap (exchange incorrect options while preserving correct answer position). We measure the flip rate---how often the model changes its answer when options are reordered.

\paragraph{Experiment 3: Evidence Conditioning.} On PubMedQA, we vary context: question-only (no context), full abstract, truncated 50\%, truncated 25\%, background-only (introduction sentences), and results-only (conclusion sentences). This tests how context completeness and type affect accuracy.

\subsection{Metrics}

We report accuracy with 95\% bootstrap confidence intervals (1,000 iterations). Position bias is computed as the absolute difference between predicted and ground truth answer distributions across positions A-D. For option-order experiments, we compute the flip rate: the proportion of questions where the model's prediction changes when options are reordered.

\section{Results}

\subsection{Prompt Ablation}

Table~\ref{tab:prompt_ablation} shows accuracy across prompting strategies. Zero-shot direct achieves the highest accuracy at 47.6\%, while chain-of-thought \textit{reduces} accuracy by 5.7 percentage points. Few-shot examples cause an even larger degradation of 11.9\%, while simultaneously increasing position bias from 0.137 to 0.472---indicating the model learns spurious patterns from examples rather than useful formats.

\begin{table}[h]
\centering
\caption{Prompt ablation results on MedMCQA (n=4,183). Random baseline is 25\%.}
\label{tab:prompt_ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Accuracy} & \textbf{95\% CI} & \textbf{Pos. Bias} \\
\midrule
Zero-shot direct & 47.6\% & [46.1\%, 49.1\%] & 0.137 \\
Zero-shot CoT & 41.9\% & [40.4\%, 43.3\%] & 0.275 \\
Few-shot direct & 35.7\% & [34.3\%, 37.0\%] & 0.472 \\
Few-shot CoT & 40.8\% & [39.4\%, 42.3\%] & 0.413 \\
Answer-only & 43.0\% & [41.5\%, 44.6\%] & 0.096 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig1_prompt_ablation.pdf}
\caption{MedGemma-4B accuracy across prompt strategies. Zero-shot direct outperforms all other strategies including chain-of-thought ($-$5.7\%) and few-shot ($-$11.9\%).}
\label{fig:prompt_ablation}
\end{figure}

\subsection{Option Order Sensitivity}

Table~\ref{tab:option_order} reveals extreme sensitivity to option ordering. The mean flip rate is 59.1\%---the model changes its answer more often than not when options are shuffled. Rotation perturbations cause the largest accuracy drops (up to 27.4\%), while distractor swaps show smaller impact ($-$8.9\%), confirming that position rather than distractor content drives fragility.

\begin{table}[h]
\centering
\caption{Option order sensitivity on MedMCQA (n=4,183). Random baseline is 25\%.}
\label{tab:option_order}
\begin{tabular}{lcc}
\toprule
\textbf{Perturbation} & \textbf{Accuracy} & \textbf{Drop} \\
\midrule
Original & 47.6\% & --- \\
Random shuffle & 29.2\% & $-$18.4\% \\
Rotate-1 & 20.2\% & $-$27.4\% \\
Rotate-2 & 24.3\% & $-$23.3\% \\
Distractor swap & 38.7\% & $-$8.9\% \\
\midrule
\textbf{Mean flip rate} & \multicolumn{2}{c}{59.1\%} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig4_option_order.pdf}
\caption{Left: Accuracy drops substantially when options are reordered. Right: The model changes its answer 59.1\% of the time when options are shuffled.}
\label{fig:option_order}
\end{figure}

\subsection{Evidence Conditioning}

Table~\ref{tab:evidence} shows context substantially affects PubMedQA performance. Most critically, truncated context performs \textit{worse} than no context: 50\% truncation yields 14.1\% (4B) and 23.4\% (27B), far below question-only baselines of 36.7\% and 31.0\%. This indicates partial context actively misleads rather than simply providing less information.

Surprisingly, MedGemma-27B achieves its best performance with results-only context (40.0\%), which \textit{exceeds} its full-context accuracy (38.2\%). The 27B model also underperforms 4B on most conditions, suggesting scale does not guarantee robustness.

\begin{table}[h]
\centering
\caption{Evidence conditioning on PubMedQA (n=1,000). Random baseline is 33.3\%.}
\label{tab:evidence}
\begin{tabular}{lcc}
\toprule
\textbf{Condition} & \textbf{MedGemma-4B} & \textbf{MedGemma-27B} \\
\midrule
Question only & 36.7\% & 31.0\% \\
Full context & 45.0\% & 38.2\% \\
Truncated 50\% & 14.1\% & 23.4\% \\
Truncated 25\% & 13.1\% & 18.6\% \\
Background only & 26.5\% & 19.8\% \\
Results only & 41.7\% & \textbf{40.0\%} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig3_evidence_conditioning.pdf}
\caption{Evidence conditioning results. Truncated context performs worse than no context (danger zone). MedGemma-27B achieves best performance with results-only context.}
\label{fig:evidence}
\end{figure}

\section{Discussion}

\subsection{Why Chain-of-Thought Hurts}

The 5.7\% accuracy drop from CoT prompting aligns with recent findings that deliberation can reduce performance on certain tasks \citep{sprague2024mind}. MedGemma was trained extensively on medical text and may have internalized domain reasoning patterns; forcing explicit step-by-step logic may override these learned patterns with less reliable deliberation.

Case-level analysis reveals the mechanism: CoT prompting changed answers on 1,262 of 4,183 questions, hurting 750 (direct correct, CoT wrong) while helping only 512---a net loss of 238 questions. The predominant failure pattern involves verbose reasoning (90.7\% exceeded 500 characters) where longer chains create opportunities for errors to compound. We also observed self-contradiction (25.6\% contained hedge words introducing conflicting logic) and confident wrong conclusions (11.1\% stated ``therefore'' before incorrect answers).

The characteristic failure mode: the model correctly identifies relevant medical concepts early in reasoning, considers alternatives, then talks itself into the wrong answer. In one case involving organophosphate poisoning, CoT correctly identified the condition and atropine's role as antidote, then continued deliberating and selected neostigmine---which would worsen the condition.

\subsection{The 59\% Flip Rate Problem}

MedGemma changes its answer 59.1\% of the time when options are shuffled, far exceeding random noise. The maximum flip rate of 72.9\% for certain perturbations means that for nearly three-quarters of questions, answers depend more on option position than content. This magnitude exceeds typical findings \citep{zheng2024llm}, suggesting medical-specialist training may not mitigate---and could exacerbate---position bias.

For clinical applications, this fragility is unacceptable. A diagnostic support system that changes recommendations based on option ordering provides no reliable signal to clinicians and undermines the premise of using AI to assist medical decision-making.

\subsection{Partial Context Actively Misleads}

Truncating context to 50\% yields 14.1\% accuracy while no context achieves 36.7\%. This 22.6 point gap suggests that partial context can mislead the model. This matters for retrieval-augmented systems in medicine, where retrieval can surface incomplete or misleading snippets. Prior work shows that models can be led astray by incorrect retrieved evidence, and that retrieval pipelines have multiple failure points that affect end-to-end quality \citep{wu2024clasheval, barnett2024rag}.

Interestingly, results-only context (41.7\% for 4B, 40.0\% for 27B) nearly matches or exceeds full context, while background-only achieves just 26.5\%/19.8\%. Both models benefit from conclusions rather than methodological background, suggesting RAG systems should prioritize high-information-density content.

\subsection{Scale and Robustness}

MedGemma-27B underperforms 4B on evidence conditioning (38.2\% vs 45.0\% with full context), demonstrating that medical benchmark performance does not scale uniformly with model size. However, 27B shows a different pattern: its best performance comes from results-only (40.0\%), exceeding full-context accuracy. This ``less is more'' finding suggests larger models may be more susceptible to distraction from verbose context but respond well to concentrated information. For deployment, this implies larger models may require selective rather than comprehensive retrieval strategies.

\section{Conclusion}

\begin{figure}[t]
\centering
\includegraphics[width=0.98\textwidth]{figures/fig6_key_findings.pdf}
\caption{Summary: CoT reduces accuracy 5.7\%; option shuffling causes 59.1\% flip rate; truncated context performs worse than no context.}
\label{fig:summary}
\end{figure}

Our evaluation reveals that standard prompt engineering techniques do not reliably improve---and may actively harm---medical question answering performance. Chain-of-thought decreases accuracy by 5.7\% while increasing position bias; few-shot examples decrease accuracy by 11.9\% while tripling position bias; shuffling options causes 59.1\% flip rates with accuracy drops up to 27.4 percentage points; and truncated context performs worse than no context.

For practitioners deploying medical LLMs, we recommend: (1) default to zero-shot direct prompting until evidence justifies added complexity; (2) test option order sensitivity before deployment and consider averaging across orderings or using debiasing techniques \citep{zheng2024llm}; (3) validate retrieval completeness for RAG systems, as incomplete context can be worse than none; and (4) for larger models, prefer selective retrieval of high-density information over comprehensive retrieval.

The extreme sensitivity to prompt variations raises fundamental questions about what benchmark accuracy measures. Before deploying medical LLMs, rigorous empirical validation on specific use cases is essential---assumed best practices from general-purpose models do not transfer.

\section*{Acknowledgments}

We thank the MedGemma team at Google for releasing open model weights. Experiments were conducted on NVIDIA A100 GPUs.

\bibliographystyle{plainnat}
\bibliography{references}

\appendix

\section{Prompt Templates}

This section provides the exact prompt templates used in our experiments to ensure reproducibility.

\subsection{Zero-shot Direct Prompt}

\begin{verbatim}
Answer the following medical question by selecting
the correct option.

Question: {question}

A. {option_a}
B. {option_b}
C. {option_c}
D. {option_d}

Answer with just the letter (A, B, C, or D):
\end{verbatim}

\subsection{Zero-shot Chain-of-Thought Prompt}

\begin{verbatim}
Answer the following medical question. Think step by step.

Question: {question}

A. {option_a}
B. {option_b}
C. {option_c}
D. {option_d}

Let's think step by step, then provide the answer
as a single letter (A, B, C, or D):
\end{verbatim}

\subsection{Few-shot Direct Prompt}

Three example questions with correct answers were prepended to each test question. Examples were randomly sampled from different medical subjects to avoid information leakage. The format follows:

\begin{verbatim}
Here are some example medical questions with answers:

Example 1:
Question: {example_1_question}
A. {ex1_a}  B. {ex1_b}  C. {ex1_c}  D. {ex1_d}
Answer: {ex1_answer}

[Examples 2-3 follow same format]

Now answer this question:
Question: {question}
A. {option_a}  B. {option_b}  C. {option_c}  D. {option_d}
Answer:
\end{verbatim}

\subsection{Answer-only Prompt}

\begin{verbatim}
{question}
A. {option_a}
B. {option_b}
C. {option_c}
D. {option_d}
\end{verbatim}

\subsection{PubMedQA Prompts}

For evidence conditioning experiments, we used the following base template:

\begin{verbatim}
Based on the following context, answer the research question.

Context: {context}

Question: {question}

Answer with one word: yes, no, or maybe.
\end{verbatim}

For the question-only condition, the context line was omitted entirely.

\section{Detailed Chain-of-Thought Analysis}

\subsection{Case-level Breakdown}

Of 4,183 MedMCQA questions evaluated with both zero-shot direct and zero-shot CoT prompting:

\begin{table}[h]
\centering
\caption{Case-level comparison of direct vs. CoT prompting}
\label{tab:cot_cases}
\begin{tabular}{lrr}
\toprule
\textbf{Outcome} & \textbf{Count} & \textbf{Percentage} \\
\midrule
Both correct & 1,511 & 36.1\% \\
Both wrong & 1,410 & 33.7\% \\
Direct correct, CoT wrong & 750 & 17.9\% \\
Direct wrong, CoT correct & 512 & 12.2\% \\
\midrule
\textbf{Net effect of CoT} & $-$238 & $-$5.7\% \\
\bottomrule
\end{tabular}
\end{table}

The net loss of 238 questions (750 hurt minus 512 helped) accounts for the 5.7\% accuracy drop observed in our main results. This demonstrates that CoT is not merely adding noise---it systematically hurts more cases than it helps.

\subsection{Failure Pattern Analysis}

We manually analyzed a random sample of 200 cases where CoT hurt performance (direct correct, CoT wrong) to identify failure patterns:

\begin{table}[h]
\centering
\caption{Failure patterns in CoT responses (n=200 sample)}
\label{tab:cot_failures}
\begin{tabular}{lrp{6cm}}
\toprule
\textbf{Pattern} & \textbf{\%} & \textbf{Description} \\
\midrule
Verbose reasoning & 90.7\% & Response exceeded 500 characters; longer deliberation correlated with errors \\
Self-contradiction & 25.6\% & Contained ``however,'' ``but,'' or ``although'' introducing conflicting logic \\
Confident wrong conclusion & 11.1\% & Stated ``therefore'' or ``thus'' before incorrect answer \\
Correct concept, wrong application & 34.5\% & Identified relevant medical concept but misapplied it \\
Distractor confusion & 28.0\% & Explicitly considered correct answer but rejected it for a distractor \\
Overthinking & 18.5\% & Simple question complicated by unnecessary considerations \\
\bottomrule
\end{tabular}
\end{table}

Note: Categories are not mutually exclusive; many failures exhibit multiple patterns.

\subsection{Illustrative Example: Organophosphate Poisoning}

\textbf{Question:} A patient presents with miosis, bradycardia, excessive salivation, and muscle fasciculations after exposure to pesticides. The most appropriate antidote is:

\textbf{Options:} A. Atropine \quad B. Pralidoxime \quad C. Neostigmine \quad D. Physostigmine

\textbf{Correct answer:} A (Atropine)

\textbf{Direct prompt response:} ``A''

\textbf{CoT prompt response (abbreviated):}
\begin{quote}
``Let me think through this step by step. The symptoms described---miosis, bradycardia, salivation, fasciculations---are classic signs of cholinergic crisis, likely from organophosphate poisoning which inhibits acetylcholinesterase...

Atropine is an anticholinergic that would block muscarinic effects. Pralidoxime reactivates acetylcholinesterase. Both are used in organophosphate poisoning...

However, neostigmine is a cholinesterase inhibitor that could... wait, let me reconsider. The question asks for an antidote. Neostigmine would actually worsen the condition by further inhibiting acetylcholinesterase...

But pralidoxime specifically targets the organophosphate-enzyme complex... Given the specific mechanism, I'll go with \textbf{C. Neostigmine}.''
\end{quote}

This example illustrates the characteristic failure mode: the model correctly identifies the medical condition and relevant pharmacology, explicitly notes that neostigmine would worsen the condition, but then talks itself into selecting it anyway after extended deliberation.

\section{Position Bias Analysis}

\subsection{Distribution Comparison}

Table~\ref{tab:position_dist} shows the distribution of correct answers in MedMCQA compared to model predictions under different prompting conditions.

\begin{table}[h]
\centering
\caption{Answer distribution: ground truth vs. predictions by condition}
\label{tab:position_dist}
\begin{tabular}{lcccc}
\toprule
\textbf{Condition} & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} \\
\midrule
Ground truth & 32.2\% & 25.1\% & 21.4\% & 21.3\% \\
\midrule
Zero-shot direct & 45.9\% & 22.1\% & 17.8\% & 14.2\% \\
Zero-shot CoT & 52.3\% & 19.4\% & 15.7\% & 12.6\% \\
Few-shot direct & 76.0\% & 12.0\% & 7.0\% & 5.0\% \\
Few-shot CoT & 68.4\% & 15.2\% & 9.8\% & 6.6\% \\
Answer-only & 41.2\% & 24.3\% & 19.1\% & 15.4\% \\
\bottomrule
\end{tabular}
\end{table}

The model shows a consistent preference for option A across all conditions, but this bias is dramatically amplified under few-shot prompting. With few-shot direct, the model predicts A for 76\% of questions despite A being correct only 32.2\% of the time---an overweight of 43.8 percentage points.

\subsection{Position Bias Score Computation}

Position bias score is computed as:
\[
\text{Bias} = \frac{1}{4} \sum_{i \in \{A,B,C,D\}} |P_{\text{pred}}(i) - P_{\text{truth}}(i)|
\]

where $P_{\text{pred}}(i)$ is the proportion of predictions for position $i$ and $P_{\text{truth}}(i)$ is the proportion of correct answers at position $i$.

\begin{table}[h]
\centering
\caption{Position bias scores by prompting condition}
\label{tab:bias_scores}
\begin{tabular}{lc}
\toprule
\textbf{Condition} & \textbf{Position Bias Score} \\
\midrule
Answer-only & 0.096 \\
Zero-shot direct & 0.137 \\
Zero-shot CoT & 0.275 \\
Few-shot CoT & 0.413 \\
Few-shot direct & 0.472 \\
\bottomrule
\end{tabular}
\end{table}

\section{Option Order Sensitivity Details}

\subsection{Flip Rate by Perturbation Type}

\begin{table}[h]
\centering
\caption{Detailed flip rates for each perturbation type}
\label{tab:flip_rates}
\begin{tabular}{lccc}
\toprule
\textbf{Perturbation} & \textbf{Flip Rate} & \textbf{Accuracy} & \textbf{$\Delta$ Accuracy} \\
\midrule
Original & --- & 47.6\% & --- \\
Distractor swap & 41.2\% & 38.7\% & $-$8.9\% \\
Random shuffle & 58.3\% & 29.2\% & $-$18.4\% \\
Rotate-2 & 63.7\% & 24.3\% & $-$23.3\% \\
Rotate-1 & 72.9\% & 20.2\% & $-$27.4\% \\
\midrule
\textbf{Mean} & 59.1\% & 28.4\% & $-$18.4\% \\
\bottomrule
\end{tabular}
\end{table}

The rotate-1 perturbation causes the highest flip rate (72.9\%) and largest accuracy drop ($-$27.4\%). This is notable because rotate-1 moves every option to an adjacent position (A$\to$B, B$\to$C, C$\to$D, D$\to$A), suggesting the model may have learned associations between specific content patterns and specific positions.

\subsection{Consistency Analysis}

We define a ``consistent'' prediction as one where the model selects the same \textit{content} (not position) across all perturbations. Only 23.4\% of questions received consistent predictions across all five conditions. This means for over three-quarters of questions, the model's answer depends on option ordering.

\begin{table}[h]
\centering
\caption{Prediction consistency across perturbations}
\label{tab:consistency}
\begin{tabular}{lr}
\toprule
\textbf{Consistency Level} & \textbf{Percentage} \\
\midrule
Consistent across all 5 conditions & 23.4\% \\
Consistent across 4 conditions & 18.7\% \\
Consistent across 3 conditions & 21.2\% \\
Consistent across 2 conditions & 19.8\% \\
Different answer for each condition & 16.9\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Evidence Conditioning Details}

\subsection{Context Length Statistics}

\begin{table}[h]
\centering
\caption{Context length statistics for PubMedQA conditions (in tokens)}
\label{tab:context_length}
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Mean} & \textbf{Median} & \textbf{Std Dev} \\
\midrule
Full context & 287.3 & 271 & 98.4 \\
Truncated 50\% & 143.6 & 135 & 49.2 \\
Truncated 25\% & 71.8 & 68 & 24.6 \\
Background only & 112.4 & 98 & 52.1 \\
Results only & 156.2 & 142 & 67.8 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Answer Distribution by Condition}

PubMedQA has three answer classes: yes, no, and maybe. Table~\ref{tab:pubmed_dist} shows how predictions vary by context condition.

\begin{table}[h]
\centering
\caption{PubMedQA prediction distribution by context condition (MedGemma-4B)}
\label{tab:pubmed_dist}
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Yes} & \textbf{No} & \textbf{Maybe} \\
\midrule
Ground truth & 55.2\% & 33.8\% & 11.0\% \\
\midrule
Question only & 62.1\% & 28.4\% & 9.5\% \\
Full context & 58.3\% & 31.2\% & 10.5\% \\
Truncated 50\% & 71.4\% & 22.1\% & 6.5\% \\
Truncated 25\% & 74.8\% & 19.7\% & 5.5\% \\
Background only & 68.2\% & 24.6\% & 7.2\% \\
Results only & 59.1\% & 30.4\% & 10.5\% \\
\bottomrule
\end{tabular}
\end{table}

Under truncation, the model becomes increasingly biased toward ``yes'' responses, suggesting it defaults to affirmative answers when context is insufficient to support nuanced reasoning.

\section{Threats to Validity}

\subsection{Answer Parsing}

Our evaluation relies on extracting answer letters from model outputs via regex patterns. We validated our parser on 500 randomly sampled responses across all conditions:

\begin{table}[h]
\centering
\caption{Parsing error rates by condition}
\label{tab:parsing}
\begin{tabular}{lc}
\toprule
\textbf{Condition} & \textbf{Parse Error Rate} \\
\midrule
Zero-shot direct & 1.4\% \\
Zero-shot CoT & 2.1\% \\
Few-shot direct & 1.6\% \\
Few-shot CoT & 1.9\% \\
Answer-only & 1.2\% \\
\bottomrule
\end{tabular}
\end{table}

CoT responses were slightly harder to parse (2.1\% vs 1.4\% for direct), but this 0.7 percentage point difference cannot explain the 5.7\% accuracy gap. Moreover, the direction of bias (CoT harder to parse) would cause us to \textit{underestimate} CoT accuracy, making our finding that CoT hurts performance more conservative.

\subsection{Dataset Imbalance}

The position bias we observe could theoretically reflect ground truth imbalance rather than model preference. However, two factors argue against this interpretation:

First, the model's overweighting of position A (45.9\% predicted vs 32.2\% true) far exceeds what ground truth would justify.

Second, our option shuffle experiments directly test for position-based predictions by tracking content across reorderings. The 59.1\% flip rate demonstrates that the model is responding to position rather than content---this finding cannot be explained by dataset imbalance.

\subsection{Dataset Contamination}

Both MedMCQA and PubMedQA are publicly available and may have been included in MedGemma's pretraining data. We cannot fully rule out contamination. However, our analysis focuses on \textit{relative} robustness across conditions rather than absolute accuracy. Even if the model has memorized answers, contamination cannot explain:

\begin{itemize}
    \item Why CoT causes accuracy to \textit{decrease} by 5.7\%
    \item Why shuffling options causes accuracy to drop by up to 27.4\%
    \item Why truncated context performs worse than no context
\end{itemize}

Memorized answers should be robust to prompt variations. The large relative degradations we observe indicate genuine sensitivity to prompt format that exists independent of potential contamination.

\subsection{Few-shot Example Selection}

We randomly sampled three example questions from different parts of the dataset and used fixed examples across all test questions. Different example selection could yield different results. Specifically:

\begin{itemize}
    \item Examples matched to test question specialty might improve performance
    \item Adversarially chosen examples might further degrade performance
    \item More examples (k $>$ 3) might help or hurt
\end{itemize}

Our results characterize few-shot prompting with arbitrary, fixed examples---a reasonable model of typical deployment but not best-case performance.

\subsection{Model-Specific Findings}

Our experiments focus primarily on MedGemma. Different model families (Med-PaLM, GPT-4, Meditron) might exhibit different sensitivity patterns. We include limited BioMistral-7B results showing even more extreme prompt sensitivity in base models, but comprehensive multi-model evaluation would strengthen generalizability claims.

\section{Limitations}

\subsection{Model Coverage}

We evaluate MedGemma-4B extensively across all experiments, MedGemma-27B on evidence conditioning only, and BioMistral-7B on prompt ablation only. The landscape of medical language models includes many other systems (Med-PaLM, Meditron, clinical GPT variants) that might exhibit different patterns. Our focus on depth over breadth means findings may not generalize to all medical LLMs.

\subsection{Quantization Constraints}

MedGemma-27B required full bfloat16 precision inference because 4-bit quantization produced NaN outputs. This limits accessibility:

\begin{itemize}
    \item Requires 80GB+ GPU memory (A100 80GB or H100)
    \item Excludes most academic researchers and smaller healthcare organizations
    \item Prevents cost-effective deployment at scale
\end{itemize}

This also suggests that quantization techniques validated on general-purpose models may fail on medical-specialist architectures---an important consideration for deployment planning.

\subsection{Task Format}

Our evaluation uses two specific formats:
\begin{itemize}
    \item Multiple-choice questions (MedMCQA)
    \item Yes/no/maybe classification (PubMedQA)
\end{itemize}

Neither format directly mirrors clinical workflows involving:
\begin{itemize}
    \item Free-text clinical notes
    \item Open-ended diagnostic reasoning
    \item Conversational patient interactions
    \item Multi-step clinical decision making
\end{itemize}

Results may not generalize to these more realistic deployment scenarios.

\subsection{Language}

Both datasets are English-only. Medical terminology, reasoning patterns, and cultural context differ across languages and healthcare systems. Our findings should not be assumed to transfer to non-English medical QA.

\subsection{Evaluation Protocol}

We evaluate single-turn question answering only. Alternative protocols might yield different results:

\begin{itemize}
    \item Multi-turn dialogue with clarification questions
    \item Self-consistency with multiple samples
    \item Chain-of-verification approaches
    \item Human-in-the-loop evaluation
\end{itemize}

\section{Computational Resources}

All experiments were conducted on NVIDIA A100 GPUs. Resource requirements:

\begin{table}[h]
\centering
\caption{Computational requirements by model}
\label{tab:compute}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{GPU Memory} & \textbf{Precision} & \textbf{Time/Question} \\
\midrule
MedGemma-4B & 16 GB & bfloat16 & $\sim$0.8s \\
MedGemma-27B & 55 GB & bfloat16 & $\sim$2.4s \\
BioMistral-7B & 14 GB & bfloat16 & $\sim$1.1s \\
\bottomrule
\end{tabular}
\end{table}

Total compute time for all experiments: approximately 72 GPU-hours on A100-80GB.

\end{document}
